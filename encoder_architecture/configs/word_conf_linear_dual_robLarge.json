{
    "model_name": "roberta-large",
    "batch_size": 8,
    "patience": 3,
    "epochs": 30,
    "runs":5,
    "init_weights":false,
    "context": "sentence",
    "desc": "word level, linear layer dual_optimizer",
    "lr1": 1e-5,
    "lr2": 1e-4,
    "layers": [{"type":"linear", "in":2048, "out":3}],
    "cls_only": false,
    "dual_optimizer": true,
    "lora": null,
    "frozen": false,
    "cls_as_context": false
}